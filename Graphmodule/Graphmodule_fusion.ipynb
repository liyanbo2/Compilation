{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.fusion import fuse_conv_bn_eval\n",
    "from torch.fx.node import Argument, Target\n",
    "from typing import Type, Dict, Any, Tuple, Iterable, Optional, List, cast\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch import fx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.fx import symbolic_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parent_name(target : str) -> Tuple[str, str]:\n",
    "    \"\"\"\n",
    "    Splits a qualname into parent path and last atom.\n",
    "    For example, `foo.bar.baz` -> (`foo.bar`, `baz`)\n",
    "    \"\"\"\n",
    "    *parent, name = target.rsplit('.', 1)\n",
    "    return parent[0] if parent else '', name "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('foo', 'bar', 'baz')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_parent_name(\"foo.bar.baz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matches_module_pattern(pattern: Iterable[Type], node: fx.Node, modules: Dict[str, Any]):\n",
    "    if len(node.args) == 0:\n",
    "        return False\n",
    "    \n",
    "    nodes: Tuple[Any, fx.Node] = (node.args[0], node)\n",
    "    for expected_type,current_node in zip(pattern,nodes):\n",
    "        if not isinstance(current_node,fx.Node):\n",
    "            return False\n",
    "        if current_node.op !=\"call_module\":\n",
    "            return False\n",
    "        if not isinstance(current_node.target,str):\n",
    "            return False\n",
    "        if current_node.target not in modules:\n",
    "            return False\n",
    "        if type(modules[current_node.target]) is not expected_type:\n",
    "            return False\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_node_module(node: fx.Node, modules: Dict[str, Any], new_module: torch.nn.Module):\n",
    "    assert(isinstance(node.target, str))\n",
    "    parent_name, name = _parent_name(node.target)\n",
    "    modules[node.target] = new_module\n",
    "    setattr(modules[parent_name], name, new_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fuse(model: torch.nn.Module, inplace=False) -> torch.nn.Module:\n",
    "    \"\"\"\n",
    "    Fuses convolution/BN layers for inference purposes. Will deepcopy your\n",
    "    model by default, but can modify the model inplace as well.\n",
    "    \"\"\"\n",
    "    patterns = [(nn.Conv1d, nn.BatchNorm1d),\n",
    "                (nn.Conv2d, nn.BatchNorm2d),\n",
    "                (nn.Conv3d, nn.BatchNorm3d)]\n",
    "    if not inplace:\n",
    "        model = copy.deepcopy(model)\n",
    "    fx_model = fx.symbolic_trace(model)\n",
    "    modules = dict(fx_model.named_modules())\n",
    "    new_graph = copy.deepcopy(fx_model.graph)\n",
    "\n",
    "    for pattern in patterns:\n",
    "        for node in new_graph.nodes:\n",
    "            if matches_module_pattern(pattern, node, modules):\n",
    "                if len(node.args[0].users) > 1:  # Output of conv is used by other nodes\n",
    "                    continue\n",
    "                conv = modules[node.args[0].target]\n",
    "                bn = modules[node.target]\n",
    "                fused_conv = fuse_conv_bn_eval(conv, bn)\n",
    "                replace_node_module(node.args[0], modules, fused_conv)\n",
    "                node.replace_all_uses_with(node.args[0])\n",
    "                new_graph.erase_node(node)\n",
    "    return fx.GraphModule(fx_model, new_graph)\n",
    "\n",
    "def remove_dropout(model: nn.Module) -> nn.Module:\n",
    "    \"\"\"\n",
    "    Removes all dropout layers from the module.\n",
    "    \"\"\"\n",
    "    fx_model = fx.symbolic_trace(model)\n",
    "\n",
    "    class DropoutRemover(torch.fx.Transformer):\n",
    "        def call_module(self, target : Target, args : Tuple[Argument, ...], kwargs : Dict[str, Any]) -> Any:\n",
    "            if isinstance(self.submodules[target], nn.Dropout):\n",
    "                assert len(args) == 1\n",
    "                return args[0]\n",
    "            else:\n",
    "                return super().call_module(target, args, kwargs)\n",
    "    return DropoutRemover(fx_model).transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestConv2d(nn.Module):\n",
    "    def __init__(self,in_channels,out_channels,**kwargs):\n",
    "        super(TestConv2d,self).__init__()\n",
    "        self.conv=nn.Conv2d(in_channels,out_channels,**kwargs)\n",
    "        self.bn=nn.BatchNorm2d(out_channels)\n",
    "        self.relu=nn.ReLU(True)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x=self.conv(x)\n",
    "        x=self.bn(x)\n",
    "        x=self.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1=TestConv2d(3,32,kernel_size=3)\n",
    "        print(self.conv1)\n",
    "        self.conv2=TestConv2d(32,64,kernel_size=3)\n",
    "        self.dropout=nn.Dropout(0.3)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x=self.conv1(x)\n",
    "        x=self.conv2(x)\n",
    "        x=self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(string,count):\n",
    "    print(f\"{'='*count}{string}{'='*count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TestConv2d(\n",
      "  (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "test_model=TestModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TestModel(\n",
       "  (conv1): TestConv2d(\n",
       "    (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "  )\n",
       "  (conv2): TestConv2d(\n",
       "    (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "  )\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================origin result====================\n",
      "graph():\n",
      "    %x : [num_users=1] = placeholder[target=x]\n",
      "    %conv1_conv : [num_users=1] = call_module[target=conv1.conv](args = (%x,), kwargs = {})\n",
      "    %conv1_bn : [num_users=1] = call_module[target=conv1.bn](args = (%conv1_conv,), kwargs = {})\n",
      "    %conv1_relu : [num_users=1] = call_module[target=conv1.relu](args = (%conv1_bn,), kwargs = {})\n",
      "    %conv2_conv : [num_users=1] = call_module[target=conv2.conv](args = (%conv1_relu,), kwargs = {})\n",
      "    %conv2_bn : [num_users=1] = call_module[target=conv2.bn](args = (%conv2_conv,), kwargs = {})\n",
      "    %conv2_relu : [num_users=1] = call_module[target=conv2.relu](args = (%conv2_bn,), kwargs = {})\n",
      "    %dropout : [num_users=1] = call_module[target=dropout](args = (%conv2_relu,), kwargs = {})\n",
      "    return dropout\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x):\n",
      "    conv1_conv = self.conv1.conv(x);  x = None\n",
      "    conv1_bn = self.conv1.bn(conv1_conv);  conv1_conv = None\n",
      "    conv1_relu = self.conv1.relu(conv1_bn);  conv1_bn = None\n",
      "    conv2_conv = self.conv2.conv(conv1_relu);  conv1_relu = None\n",
      "    conv2_bn = self.conv2.bn(conv2_conv);  conv2_conv = None\n",
      "    conv2_relu = self.conv2.relu(conv2_bn);  conv2_bn = None\n",
      "    dropout = self.dropout(conv2_relu);  conv2_relu = None\n",
      "    return dropout\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "origin_model=symbolic_trace(test_model)\n",
    "show(\"origin result\",20)\n",
    "print(origin_model.graph)\n",
    "print(origin_model.code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================fuse result====================\n",
      "graph():\n",
      "    %x : [num_users=1] = placeholder[target=x]\n",
      "    %conv1_conv : [num_users=1] = call_module[target=conv1.conv](args = (%x,), kwargs = {})\n",
      "    %conv1_bn : [num_users=1] = call_module[target=conv1.bn](args = (%conv1_conv,), kwargs = {})\n",
      "    %conv1_relu : [num_users=1] = call_module[target=conv1.relu](args = (%conv1_bn,), kwargs = {})\n",
      "    %conv2_conv : [num_users=1] = call_module[target=conv2.conv](args = (%conv1_relu,), kwargs = {})\n",
      "    %conv2_bn : [num_users=1] = call_module[target=conv2.bn](args = (%conv2_conv,), kwargs = {})\n",
      "    %conv2_relu : [num_users=1] = call_module[target=conv2.relu](args = (%conv2_bn,), kwargs = {})\n",
      "    return conv2_relu\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x):\n",
      "    conv1_conv = self.conv1.conv(x);  x = None\n",
      "    conv1_bn = self.conv1.bn(conv1_conv);  conv1_conv = None\n",
      "    conv1_relu = self.conv1.relu(conv1_bn);  conv1_bn = None\n",
      "    conv2_conv = self.conv2.conv(conv1_relu);  conv1_relu = None\n",
      "    conv2_bn = self.conv2.bn(conv2_conv);  conv2_conv = None\n",
      "    conv2_relu = self.conv2.relu(conv2_bn);  conv2_bn = None\n",
      "    return conv2_relu\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "fuse_model=fuse(test_model)\n",
    "fuse_model=remove_dropout(fuse_model)\n",
    "show(\"fuse result\",20)\n",
    "print(fuse_model.graph)\n",
    "print(fuse_model.code)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
