{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from time import perf_counter\n",
    "import numpy as np\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.20s/it]\n"
     ]
    }
   ],
   "source": [
    "model_name = '/dataset/crosspipe/llama-2-chat/Llama-2-7b-chat-hf'\n",
    "model = LlamaForCausalLM.from_pretrained(model_name).cuda()\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Hello, my name is Llama.\"\n",
    "inputs = tokenizer(text, return_tensors='pt').to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 包装模型的前向传播，简化输出\n",
    "class WrappedModel(torch.nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(WrappedModel, self).__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        outputs = self.model(input_ids)\n",
    "        return outputs.logits\n",
    "\n",
    "wrapped_model = WrappedModel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WrappedModel(\n",
       "  (model): LlamaForCausalLM(\n",
       "    (model): LlamaModel(\n",
       "      (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
       "      (layers): ModuleList(\n",
       "        (0-31): 32 x LlamaDecoderLayer(\n",
       "          (self_attn): LlamaAttention(\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (rotary_emb): LlamaRotaryEmbedding()\n",
       "          )\n",
       "          (mlp): LlamaMLP(\n",
       "            (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "            (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "            (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "            (act_fn): SiLUActivation()\n",
       "          )\n",
       "          (input_layernorm): LlamaRMSNorm()\n",
       "          (post_attention_layernorm): LlamaRMSNorm()\n",
       "        )\n",
       "      )\n",
       "      (norm): LlamaRMSNorm()\n",
       "    )\n",
       "    (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapped_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py:579: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if input_shape[-1] > 1:\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py:119: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if seq_len > self.max_seq_len_cached:\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py:332: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attn_weights.size() != (bsz, self.num_heads, q_len, kv_seq_len):\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py:339: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attention_mask.size() != (bsz, 1, q_len, kv_seq_len):\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py:349: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):\n"
     ]
    }
   ],
   "source": [
    "traced_model = torch.jit.trace(wrapped_model, (inputs.input_ids,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WrappedModel(\n",
       "  original_name=WrappedModel\n",
       "  (model): LlamaForCausalLM(\n",
       "    original_name=LlamaForCausalLM\n",
       "    (model): LlamaModel(\n",
       "      original_name=LlamaModel\n",
       "      (embed_tokens): Embedding(original_name=Embedding)\n",
       "      (layers): ModuleList(\n",
       "        original_name=ModuleList\n",
       "        (0): LlamaDecoderLayer(\n",
       "          original_name=LlamaDecoderLayer\n",
       "          (self_attn): LlamaAttention(\n",
       "            original_name=LlamaAttention\n",
       "            (q_proj): Linear(original_name=Linear)\n",
       "            (k_proj): Linear(original_name=Linear)\n",
       "            (v_proj): Linear(original_name=Linear)\n",
       "            (o_proj): Linear(original_name=Linear)\n",
       "            (rotary_emb): LlamaRotaryEmbedding(original_name=LlamaRotaryEmbedding)\n",
       "          )\n",
       "          (mlp): LlamaMLP(\n",
       "            original_name=LlamaMLP\n",
       "            (gate_proj): Linear(original_name=Linear)\n",
       "            (up_proj): Linear(original_name=Linear)\n",
       "            (down_proj): Linear(original_name=Linear)\n",
       "            (act_fn): SiLUActivation(original_name=SiLUActivation)\n",
       "          )\n",
       "          (input_layernorm): LlamaRMSNorm(original_name=LlamaRMSNorm)\n",
       "          (post_attention_layernorm): LlamaRMSNorm(original_name=LlamaRMSNorm)\n",
       "        )\n",
       "        (1): LlamaDecoderLayer(\n",
       "          original_name=LlamaDecoderLayer\n",
       "          (self_attn): LlamaAttention(\n",
       "            original_name=LlamaAttention\n",
       "            (q_proj): Linear(original_name=Linear)\n",
       "            (k_proj): Linear(original_name=Linear)\n",
       "            (v_proj): Linear(original_name=Linear)\n",
       "            (o_proj): Linear(original_name=Linear)\n",
       "            (rotary_emb): LlamaRotaryEmbedding(original_name=LlamaRotaryEmbedding)\n",
       "          )\n",
       "          (mlp): LlamaMLP(\n",
       "            original_name=LlamaMLP\n",
       "            (gate_proj): Linear(original_name=Linear)\n",
       "            (up_proj): Linear(original_name=Linear)\n",
       "            (down_proj): Linear(original_name=Linear)\n",
       "            (act_fn): SiLUActivation(original_name=SiLUActivation)\n",
       "          )\n",
       "          (input_layernorm): LlamaRMSNorm(original_name=LlamaRMSNorm)\n",
       "          (post_attention_layernorm): LlamaRMSNorm(original_name=LlamaRMSNorm)\n",
       "        )\n",
       "        (2): LlamaDecoderLayer(\n",
       "          original_name=LlamaDecoderLayer\n",
       "          (self_attn): LlamaAttention(\n",
       "            original_name=LlamaAttention\n",
       "            (q_proj): Linear(original_name=Linear)\n",
       "            (k_proj): Linear(original_name=Linear)\n",
       "            (v_proj): Linear(original_name=Linear)\n",
       "            (o_proj): Linear(original_name=Linear)\n",
       "            (rotary_emb): LlamaRotaryEmbedding(original_name=LlamaRotaryEmbedding)\n",
       "          )\n",
       "          (mlp): LlamaMLP(\n",
       "            original_name=LlamaMLP\n",
       "            (gate_proj): Linear(original_name=Linear)\n",
       "            (up_proj): Linear(original_name=Linear)\n",
       "            (down_proj): Linear(original_name=Linear)\n",
       "            (act_fn): SiLUActivation(original_name=SiLUActivation)\n",
       "          )\n",
       "          (input_layernorm): LlamaRMSNorm(original_name=LlamaRMSNorm)\n",
       "          (post_attention_layernorm): LlamaRMSNorm(original_name=LlamaRMSNorm)\n",
       "        )\n",
       "        (3): LlamaDecoderLayer(\n",
       "          original_name=LlamaDecoderLayer\n",
       "          (self_attn): LlamaAttention(\n",
       "            original_name=LlamaAttention\n",
       "            (q_proj): Linear(original_name=Linear)\n",
       "            (k_proj): Linear(original_name=Linear)\n",
       "            (v_proj): Linear(original_name=Linear)\n",
       "            (o_proj): Linear(original_name=Linear)\n",
       "            (rotary_emb): LlamaRotaryEmbedding(original_name=LlamaRotaryEmbedding)\n",
       "          )\n",
       "          (mlp): LlamaMLP(\n",
       "            original_name=LlamaMLP\n",
       "            (gate_proj): Linear(original_name=Linear)\n",
       "            (up_proj): Linear(original_name=Linear)\n",
       "            (down_proj): Linear(original_name=Linear)\n",
       "            (act_fn): SiLUActivation(original_name=SiLUActivation)\n",
       "          )\n",
       "          (input_layernorm): LlamaRMSNorm(original_name=LlamaRMSNorm)\n",
       "          (post_attention_layernorm): LlamaRMSNorm(original_name=LlamaRMSNorm)\n",
       "        )\n",
       "        (4): LlamaDecoderLayer(\n",
       "          original_name=LlamaDecoderLayer\n",
       "          (self_attn): LlamaAttention(\n",
       "            original_name=LlamaAttention\n",
       "            (q_proj): Linear(original_name=Linear)\n",
       "            (k_proj): Linear(original_name=Linear)\n",
       "            (v_proj): Linear(original_name=Linear)\n",
       "            (o_proj): Linear(original_name=Linear)\n",
       "            (rotary_emb): LlamaRotaryEmbedding(original_name=LlamaRotaryEmbedding)\n",
       "          )\n",
       "          (mlp): LlamaMLP(\n",
       "            original_name=LlamaMLP\n",
       "            (gate_proj): Linear(original_name=Linear)\n",
       "            (up_proj): Linear(original_name=Linear)\n",
       "            (down_proj): Linear(original_name=Linear)\n",
       "            (act_fn): SiLUActivation(original_name=SiLUActivation)\n",
       "          )\n",
       "          (input_layernorm): LlamaRMSNorm(original_name=LlamaRMSNorm)\n",
       "          (post_attention_layernorm): LlamaRMSNorm(original_name=LlamaRMSNorm)\n",
       "        )\n",
       "        (5): LlamaDecoderLayer(\n",
       "          original_name=LlamaDecoderLayer\n",
       "          (self_attn): LlamaAttention(\n",
       "            original_name=LlamaAttention\n",
       "            (q_proj): Linear(original_name=Linear)\n",
       "            (k_proj): Linear(original_name=Linear)\n",
       "            (v_proj): Linear(original_name=Linear)\n",
       "            (o_proj): Linear(original_name=Linear)\n",
       "            (rotary_emb): LlamaRotaryEmbedding(original_name=LlamaRotaryEmbedding)\n",
       "          )\n",
       "          (mlp): LlamaMLP(\n",
       "            original_name=LlamaMLP\n",
       "            (gate_proj): Linear(original_name=Linear)\n",
       "            (up_proj): Linear(original_name=Linear)\n",
       "            (down_proj): Linear(original_name=Linear)\n",
       "            (act_fn): SiLUActivation(original_name=SiLUActivation)\n",
       "          )\n",
       "          (input_layernorm): LlamaRMSNorm(original_name=LlamaRMSNorm)\n",
       "          (post_attention_layernorm): LlamaRMSNorm(original_name=LlamaRMSNorm)\n",
       "        )\n",
       "        (6): LlamaDecoderLayer(\n",
       "          original_name=LlamaDecoderLayer\n",
       "          (self_attn): LlamaAttention(\n",
       "            original_name=LlamaAttention\n",
       "            (q_proj): Linear(original_name=Linear)\n",
       "            (k_proj): Linear(original_name=Linear)\n",
       "            (v_proj): Linear(original_name=Linear)\n",
       "            (o_proj): Linear(original_name=Linear)\n",
       "            (rotary_emb): LlamaRotaryEmbedding(original_name=LlamaRotaryEmbedding)\n",
       "          )\n",
       "          (mlp): LlamaMLP(\n",
       "            original_name=LlamaMLP\n",
       "            (gate_proj): Linear(original_name=Linear)\n",
       "            (up_proj): Linear(original_name=Linear)\n",
       "            (down_proj): Linear(original_name=Linear)\n",
       "            (act_fn): SiLUActivation(original_name=SiLUActivation)\n",
       "          )\n",
       "          (input_layernorm): LlamaRMSNorm(original_name=LlamaRMSNorm)\n",
       "          (post_attention_layernorm): LlamaRMSNorm(original_name=LlamaRMSNorm)\n",
       "        )\n",
       "        (7): LlamaDecoderLayer(\n",
       "          original_name=LlamaDecoderLayer\n",
       "          (self_attn): LlamaAttention(\n",
       "            original_name=LlamaAttention\n",
       "            (q_proj): Linear(original_name=Linear)\n",
       "            (k_proj): Linear(original_name=Linear)\n",
       "            (v_proj): Linear(original_name=Linear)\n",
       "            (o_proj): Linear(original_name=Linear)\n",
       "            (rotary_emb): LlamaRotaryEmbedding(original_name=LlamaRotaryEmbedding)\n",
       "          )\n",
       "          (mlp): LlamaMLP(\n",
       "            original_name=LlamaMLP\n",
       "            (gate_proj): Linear(original_name=Linear)\n",
       "            (up_proj): Linear(original_name=Linear)\n",
       "            (down_proj): Linear(original_name=Linear)\n",
       "            (act_fn): SiLUActivation(original_name=SiLUActivation)\n",
       "          )\n",
       "          (input_layernorm): LlamaRMSNorm(original_name=LlamaRMSNorm)\n",
       "          (post_attention_layernorm): LlamaRMSNorm(original_name=LlamaRMSNorm)\n",
       "        )\n",
       "        (8): LlamaDecoderLayer(\n",
       "          original_name=LlamaDecoderLayer\n",
       "          (self_attn): LlamaAttention(\n",
       "            original_name=LlamaAttention\n",
       "            (q_proj): Linear(original_name=Linear)\n",
       "            (k_proj): Linear(original_name=Linear)\n",
       "            (v_proj): Linear(original_name=Linear)\n",
       "            (o_proj): Linear(original_name=Linear)\n",
       "            (rotary_emb): LlamaRotaryEmbedding(original_name=LlamaRotaryEmbedding)\n",
       "          )\n",
       "          (mlp): LlamaMLP(\n",
       "            original_name=LlamaMLP\n",
       "            (gate_proj): Linear(original_name=Linear)\n",
       "            (up_proj): Linear(original_name=Linear)\n",
       "            (down_proj): Linear(original_name=Linear)\n",
       "            (act_fn): SiLUActivation(original_name=SiLUActivation)\n",
       "          )\n",
       "          (input_layernorm): LlamaRMSNorm(original_name=LlamaRMSNorm)\n",
       "          (post_attention_layernorm): LlamaRMSNorm(original_name=LlamaRMSNorm)\n",
       "        )\n",
       "        (9): LlamaDecoderLayer(\n",
       "          original_name=LlamaDecoderLayer\n",
       "          (self_attn): LlamaAttention(\n",
       "            original_name=LlamaAttention\n",
       "            (q_proj): Linear(original_name=Linear)\n",
       "            (k_proj): Linear(original_name=Linear)\n",
       "            (v_proj): Linear(original_name=Linear)\n",
       "            (o_proj): Linear(original_name=Linear)\n",
       "            (rotary_emb): LlamaRotaryEmbedding(original_name=LlamaRotaryEmbedding)\n",
       "          )\n",
       "          (mlp): LlamaMLP(\n",
       "            original_name=LlamaMLP\n",
       "            (gate_proj): Linear(original_name=Linear)\n",
       "            (up_proj): Linear(original_name=Linear)\n",
       "            (down_proj): Linear(original_name=Linear)\n",
       "            (act_fn): SiLUActivation(original_name=SiLUActivation)\n",
       "          )\n",
       "          (input_layernorm): LlamaRMSNorm(original_name=LlamaRMSNorm)\n",
       "          (post_attention_layernorm): LlamaRMSNorm(original_name=LlamaRMSNorm)\n",
       "        )\n",
       "        (10): LlamaDecoderLayer(\n",
       "          original_name=LlamaDecoderLayer\n",
       "          (self_attn): LlamaAttention(\n",
       "            original_name=LlamaAttention\n",
       "            (q_proj): Linear(original_name=Linear)\n",
       "            (k_proj): Linear(original_name=Linear)\n",
       "            (v_proj): Linear(original_name=Linear)\n",
       "            (o_proj): Linear(original_name=Linear)\n",
       "            (rotary_emb): LlamaRotaryEmbedding(original_name=LlamaRotaryEmbedding)\n",
       "          )\n",
       "          (mlp): LlamaMLP(\n",
       "            original_name=LlamaMLP\n",
       "            (gate_proj): Linear(original_name=Linear)\n",
       "            (up_proj): Linear(original_name=Linear)\n",
       "            (down_proj): Linear(original_name=Linear)\n",
       "            (act_fn): SiLUActivation(original_name=SiLUActivation)\n",
       "          )\n",
       "          (input_layernorm): LlamaRMSNorm(original_name=LlamaRMSNorm)\n",
       "          (post_attention_layernorm): LlamaRMSNorm(original_name=LlamaRMSNorm)\n",
       "        )\n",
       "        (11): LlamaDecoderLayer(\n",
       "          original_name=LlamaDecoderLayer\n",
       "          (self_attn): LlamaAttention(\n",
       "            original_name=LlamaAttention\n",
       "            (q_proj): Linear(original_name=Linear)\n",
       "            (k_proj): Linear(original_name=Linear)\n",
       "            (v_proj): Linear(original_name=Linear)\n",
       "            (o_proj): Linear(original_name=Linear)\n",
       "            (rotary_emb): LlamaRotaryEmbedding(original_name=LlamaRotaryEmbedding)\n",
       "          )\n",
       "          (mlp): LlamaMLP(\n",
       "            original_name=LlamaMLP\n",
       "            (gate_proj): Linear(original_name=Linear)\n",
       "            (up_proj): Linear(original_name=Linear)\n",
       "            (down_proj): Linear(original_name=Linear)\n",
       "            (act_fn): SiLUActivation(original_name=SiLUActivation)\n",
       "          )\n",
       "          (input_layernorm): LlamaRMSNorm(original_name=LlamaRMSNorm)\n",
       "          (post_attention_layernorm): LlamaRMSNorm(original_name=LlamaRMSNorm)\n",
       "        )\n",
       "        (12): LlamaDecoderLayer(\n",
       "          original_name=LlamaDecoderLayer\n",
       "          (self_attn): LlamaAttention(\n",
       "            original_name=LlamaAttention\n",
       "            (q_proj): Linear(original_name=Linear)\n",
       "            (k_proj): Linear(original_name=Linear)\n",
       "            (v_proj): Linear(original_name=Linear)\n",
       "            (o_proj): Linear(original_name=Linear)\n",
       "            (rotary_emb): LlamaRotaryEmbedding(original_name=LlamaRotaryEmbedding)\n",
       "          )\n",
       "          (mlp): LlamaMLP(\n",
       "            original_name=LlamaMLP\n",
       "            (gate_proj): Linear(original_name=Linear)\n",
       "            (up_proj): Linear(original_name=Linear)\n",
       "            (down_proj): Linear(original_name=Linear)\n",
       "            (act_fn): SiLUActivation(original_name=SiLUActivation)\n",
       "          )\n",
       "          (input_layernorm): LlamaRMSNorm(original_name=LlamaRMSNorm)\n",
       "          (post_attention_layernorm): LlamaRMSNorm(original_name=LlamaRMSNorm)\n",
       "        )\n",
       "        (13): LlamaDecoderLayer(\n",
       "          original_name=LlamaDecoderLayer\n",
       "          (self_attn): LlamaAttention(\n",
       "            original_name=LlamaAttention\n",
       "            (q_proj): Linear(original_name=Linear)\n",
       "            (k_proj): Linear(original_name=Linear)\n",
       "            (v_proj): Linear(original_name=Linear)\n",
       "            (o_proj): Linear(original_name=Linear)\n",
       "            (rotary_emb): LlamaRotaryEmbedding(original_name=LlamaRotaryEmbedding)\n",
       "          )\n",
       "          (mlp): LlamaMLP(\n",
       "            original_name=LlamaMLP\n",
       "            (gate_proj): Linear(original_name=Linear)\n",
       "            (up_proj): Linear(original_name=Linear)\n",
       "            (down_proj): Linear(original_name=Linear)\n",
       "            (act_fn): SiLUActivation(original_name=SiLUActivation)\n",
       "          )\n",
       "          (input_layernorm): LlamaRMSNorm(original_name=LlamaRMSNorm)\n",
       "          (post_attention_layernorm): LlamaRMSNorm(original_name=LlamaRMSNorm)\n",
       "        )\n",
       "        (14): LlamaDecoderLayer(\n",
       "          original_name=LlamaDecoderLayer\n",
       "          (self_attn): LlamaAttention(\n",
       "            original_name=LlamaAttention\n",
       "            (q_proj): Linear(original_name=Linear)\n",
       "            (k_proj): Linear(original_name=Linear)\n",
       "            (v_proj): Linear(original_name=Linear)\n",
       "            (o_proj): Linear(original_name=Linear)\n",
       "            (rotary_emb): LlamaRotaryEmbedding(original_name=LlamaRotaryEmbedding)\n",
       "          )\n",
       "          (mlp): LlamaMLP(\n",
       "            original_name=LlamaMLP\n",
       "            (gate_proj): Linear(original_name=Linear)\n",
       "            (up_proj): Linear(original_name=Linear)\n",
       "            (down_proj): Linear(original_name=Linear)\n",
       "            (act_fn): SiLUActivation(original_name=SiLUActivation)\n",
       "          )\n",
       "          (input_layernorm): LlamaRMSNorm(original_name=LlamaRMSNorm)\n",
       "          (post_attention_layernorm): LlamaRMSNorm(original_name=LlamaRMSNorm)\n",
       "        )\n",
       "        (15): LlamaDecoderLayer(\n",
       "          original_name=LlamaDecoderLayer\n",
       "          (self_attn): LlamaAttention(\n",
       "            original_name=LlamaAttention\n",
       "            (q_proj): Linear(original_name=Linear)\n",
       "            (k_proj): Linear(original_name=Linear)\n",
       "            (v_proj): Linear(original_name=Linear)\n",
       "            (o_proj): Linear(original_name=Linear)\n",
       "            (rotary_emb): LlamaRotaryEmbedding(original_name=LlamaRotaryEmbedding)\n",
       "          )\n",
       "          (mlp): LlamaMLP(\n",
       "            original_name=LlamaMLP\n",
       "            (gate_proj): Linear(original_name=Linear)\n",
       "            (up_proj): Linear(original_name=Linear)\n",
       "            (down_proj): Linear(original_name=Linear)\n",
       "            (act_fn): SiLUActivation(original_name=SiLUActivation)\n",
       "          )\n",
       "          (input_layernorm): LlamaRMSNorm(original_name=LlamaRMSNorm)\n",
       "          (post_attention_layernorm): LlamaRMSNorm(original_name=LlamaRMSNorm)\n",
       "        )\n",
       "        (16): LlamaDecoderLayer(\n",
       "          original_name=LlamaDecoderLayer\n",
       "          (self_attn): LlamaAttention(\n",
       "            original_name=LlamaAttention\n",
       "            (q_proj): Linear(original_name=Linear)\n",
       "            (k_proj): Linear(original_name=Linear)\n",
       "            (v_proj): Linear(original_name=Linear)\n",
       "            (o_proj): Linear(original_name=Linear)\n",
       "            (rotary_emb): LlamaRotaryEmbedding(original_name=LlamaRotaryEmbedding)\n",
       "          )\n",
       "          (mlp): LlamaMLP(\n",
       "            original_name=LlamaMLP\n",
       "            (gate_proj): Linear(original_name=Linear)\n",
       "            (up_proj): Linear(original_name=Linear)\n",
       "            (down_proj): Linear(original_name=Linear)\n",
       "            (act_fn): SiLUActivation(original_name=SiLUActivation)\n",
       "          )\n",
       "          (input_layernorm): LlamaRMSNorm(original_name=LlamaRMSNorm)\n",
       "          (post_attention_layernorm): LlamaRMSNorm(original_name=LlamaRMSNorm)\n",
       "        )\n",
       "        (17): LlamaDecoderLayer(\n",
       "          original_name=LlamaDecoderLayer\n",
       "          (self_attn): LlamaAttention(\n",
       "            original_name=LlamaAttention\n",
       "            (q_proj): Linear(original_name=Linear)\n",
       "            (k_proj): Linear(original_name=Linear)\n",
       "            (v_proj): Linear(original_name=Linear)\n",
       "            (o_proj): Linear(original_name=Linear)\n",
       "            (rotary_emb): LlamaRotaryEmbedding(original_name=LlamaRotaryEmbedding)\n",
       "          )\n",
       "          (mlp): LlamaMLP(\n",
       "            original_name=LlamaMLP\n",
       "            (gate_proj): Linear(original_name=Linear)\n",
       "            (up_proj): Linear(original_name=Linear)\n",
       "            (down_proj): Linear(original_name=Linear)\n",
       "            (act_fn): SiLUActivation(original_name=SiLUActivation)\n",
       "          )\n",
       "          (input_layernorm): LlamaRMSNorm(original_name=LlamaRMSNorm)\n",
       "          (post_attention_layernorm): LlamaRMSNorm(original_name=LlamaRMSNorm)\n",
       "        )\n",
       "        (18): LlamaDecoderLayer(\n",
       "          original_name=LlamaDecoderLayer\n",
       "          (self_attn): LlamaAttention(\n",
       "            original_name=LlamaAttention\n",
       "            (q_proj): Linear(original_name=Linear)\n",
       "            (k_proj): Linear(original_name=Linear)\n",
       "            (v_proj): Linear(original_name=Linear)\n",
       "            (o_proj): Linear(original_name=Linear)\n",
       "            (rotary_emb): LlamaRotaryEmbedding(original_name=LlamaRotaryEmbedding)\n",
       "          )\n",
       "          (mlp): LlamaMLP(\n",
       "            original_name=LlamaMLP\n",
       "            (gate_proj): Linear(original_name=Linear)\n",
       "            (up_proj): Linear(original_name=Linear)\n",
       "            (down_proj): Linear(original_name=Linear)\n",
       "            (act_fn): SiLUActivation(original_name=SiLUActivation)\n",
       "          )\n",
       "          (input_layernorm): LlamaRMSNorm(original_name=LlamaRMSNorm)\n",
       "          (post_attention_layernorm): LlamaRMSNorm(original_name=LlamaRMSNorm)\n",
       "        )\n",
       "        (19): LlamaDecoderLayer(\n",
       "          original_name=LlamaDecoderLayer\n",
       "          (self_attn): LlamaAttention(\n",
       "            original_name=LlamaAttention\n",
       "            (q_proj): Linear(original_name=Linear)\n",
       "            (k_proj): Linear(original_name=Linear)\n",
       "            (v_proj): Linear(original_name=Linear)\n",
       "            (o_proj): Linear(original_name=Linear)\n",
       "            (rotary_emb): LlamaRotaryEmbedding(original_name=LlamaRotaryEmbedding)\n",
       "          )\n",
       "          (mlp): LlamaMLP(\n",
       "            original_name=LlamaMLP\n",
       "            (gate_proj): Linear(original_name=Linear)\n",
       "            (up_proj): Linear(original_name=Linear)\n",
       "            (down_proj): Linear(original_name=Linear)\n",
       "            (act_fn): SiLUActivation(original_name=SiLUActivation)\n",
       "          )\n",
       "          (input_layernorm): LlamaRMSNorm(original_name=LlamaRMSNorm)\n",
       "          (post_attention_layernorm): LlamaRMSNorm(original_name=LlamaRMSNorm)\n",
       "        )\n",
       "        (20): LlamaDecoderLayer(\n",
       "          original_name=LlamaDecoderLayer\n",
       "          (self_attn): LlamaAttention(\n",
       "            original_name=LlamaAttention\n",
       "            (q_proj): Linear(original_name=Linear)\n",
       "            (k_proj): Linear(original_name=Linear)\n",
       "            (v_proj): Linear(original_name=Linear)\n",
       "            (o_proj): Linear(original_name=Linear)\n",
       "            (rotary_emb): LlamaRotaryEmbedding(original_name=LlamaRotaryEmbedding)\n",
       "          )\n",
       "          (mlp): LlamaMLP(\n",
       "            original_name=LlamaMLP\n",
       "            (gate_proj): Linear(original_name=Linear)\n",
       "            (up_proj): Linear(original_name=Linear)\n",
       "            (down_proj): Linear(original_name=Linear)\n",
       "            (act_fn): SiLUActivation(original_name=SiLUActivation)\n",
       "          )\n",
       "          (input_layernorm): LlamaRMSNorm(original_name=LlamaRMSNorm)\n",
       "          (post_attention_layernorm): LlamaRMSNorm(original_name=LlamaRMSNorm)\n",
       "        )\n",
       "        (21): LlamaDecoderLayer(\n",
       "          original_name=LlamaDecoderLayer\n",
       "          (self_attn): LlamaAttention(\n",
       "            original_name=LlamaAttention\n",
       "            (q_proj): Linear(original_name=Linear)\n",
       "            (k_proj): Linear(original_name=Linear)\n",
       "            (v_proj): Linear(original_name=Linear)\n",
       "            (o_proj): Linear(original_name=Linear)\n",
       "            (rotary_emb): LlamaRotaryEmbedding(original_name=LlamaRotaryEmbedding)\n",
       "          )\n",
       "          (mlp): LlamaMLP(\n",
       "            original_name=LlamaMLP\n",
       "            (gate_proj): Linear(original_name=Linear)\n",
       "            (up_proj): Linear(original_name=Linear)\n",
       "            (down_proj): Linear(original_name=Linear)\n",
       "            (act_fn): SiLUActivation(original_name=SiLUActivation)\n",
       "          )\n",
       "          (input_layernorm): LlamaRMSNorm(original_name=LlamaRMSNorm)\n",
       "          (post_attention_layernorm): LlamaRMSNorm(original_name=LlamaRMSNorm)\n",
       "        )\n",
       "        (22): LlamaDecoderLayer(\n",
       "          original_name=LlamaDecoderLayer\n",
       "          (self_attn): LlamaAttention(\n",
       "            original_name=LlamaAttention\n",
       "            (q_proj): Linear(original_name=Linear)\n",
       "            (k_proj): Linear(original_name=Linear)\n",
       "            (v_proj): Linear(original_name=Linear)\n",
       "            (o_proj): Linear(original_name=Linear)\n",
       "            (rotary_emb): LlamaRotaryEmbedding(original_name=LlamaRotaryEmbedding)\n",
       "          )\n",
       "          (mlp): LlamaMLP(\n",
       "            original_name=LlamaMLP\n",
       "            (gate_proj): Linear(original_name=Linear)\n",
       "            (up_proj): Linear(original_name=Linear)\n",
       "            (down_proj): Linear(original_name=Linear)\n",
       "            (act_fn): SiLUActivation(original_name=SiLUActivation)\n",
       "          )\n",
       "          (input_layernorm): LlamaRMSNorm(original_name=LlamaRMSNorm)\n",
       "          (post_attention_layernorm): LlamaRMSNorm(original_name=LlamaRMSNorm)\n",
       "        )\n",
       "        (23): LlamaDecoderLayer(\n",
       "          original_name=LlamaDecoderLayer\n",
       "          (self_attn): LlamaAttention(\n",
       "            original_name=LlamaAttention\n",
       "            (q_proj): Linear(original_name=Linear)\n",
       "            (k_proj): Linear(original_name=Linear)\n",
       "            (v_proj): Linear(original_name=Linear)\n",
       "            (o_proj): Linear(original_name=Linear)\n",
       "            (rotary_emb): LlamaRotaryEmbedding(original_name=LlamaRotaryEmbedding)\n",
       "          )\n",
       "          (mlp): LlamaMLP(\n",
       "            original_name=LlamaMLP\n",
       "            (gate_proj): Linear(original_name=Linear)\n",
       "            (up_proj): Linear(original_name=Linear)\n",
       "            (down_proj): Linear(original_name=Linear)\n",
       "            (act_fn): SiLUActivation(original_name=SiLUActivation)\n",
       "          )\n",
       "          (input_layernorm): LlamaRMSNorm(original_name=LlamaRMSNorm)\n",
       "          (post_attention_layernorm): LlamaRMSNorm(original_name=LlamaRMSNorm)\n",
       "        )\n",
       "        (24): LlamaDecoderLayer(\n",
       "          original_name=LlamaDecoderLayer\n",
       "          (self_attn): LlamaAttention(\n",
       "            original_name=LlamaAttention\n",
       "            (q_proj): Linear(original_name=Linear)\n",
       "            (k_proj): Linear(original_name=Linear)\n",
       "            (v_proj): Linear(original_name=Linear)\n",
       "            (o_proj): Linear(original_name=Linear)\n",
       "            (rotary_emb): LlamaRotaryEmbedding(original_name=LlamaRotaryEmbedding)\n",
       "          )\n",
       "          (mlp): LlamaMLP(\n",
       "            original_name=LlamaMLP\n",
       "            (gate_proj): Linear(original_name=Linear)\n",
       "            (up_proj): Linear(original_name=Linear)\n",
       "            (down_proj): Linear(original_name=Linear)\n",
       "            (act_fn): SiLUActivation(original_name=SiLUActivation)\n",
       "          )\n",
       "          (input_layernorm): LlamaRMSNorm(original_name=LlamaRMSNorm)\n",
       "          (post_attention_layernorm): LlamaRMSNorm(original_name=LlamaRMSNorm)\n",
       "        )\n",
       "        (25): LlamaDecoderLayer(\n",
       "          original_name=LlamaDecoderLayer\n",
       "          (self_attn): LlamaAttention(\n",
       "            original_name=LlamaAttention\n",
       "            (q_proj): Linear(original_name=Linear)\n",
       "            (k_proj): Linear(original_name=Linear)\n",
       "            (v_proj): Linear(original_name=Linear)\n",
       "            (o_proj): Linear(original_name=Linear)\n",
       "            (rotary_emb): LlamaRotaryEmbedding(original_name=LlamaRotaryEmbedding)\n",
       "          )\n",
       "          (mlp): LlamaMLP(\n",
       "            original_name=LlamaMLP\n",
       "            (gate_proj): Linear(original_name=Linear)\n",
       "            (up_proj): Linear(original_name=Linear)\n",
       "            (down_proj): Linear(original_name=Linear)\n",
       "            (act_fn): SiLUActivation(original_name=SiLUActivation)\n",
       "          )\n",
       "          (input_layernorm): LlamaRMSNorm(original_name=LlamaRMSNorm)\n",
       "          (post_attention_layernorm): LlamaRMSNorm(original_name=LlamaRMSNorm)\n",
       "        )\n",
       "        (26): LlamaDecoderLayer(\n",
       "          original_name=LlamaDecoderLayer\n",
       "          (self_attn): LlamaAttention(\n",
       "            original_name=LlamaAttention\n",
       "            (q_proj): Linear(original_name=Linear)\n",
       "            (k_proj): Linear(original_name=Linear)\n",
       "            (v_proj): Linear(original_name=Linear)\n",
       "            (o_proj): Linear(original_name=Linear)\n",
       "            (rotary_emb): LlamaRotaryEmbedding(original_name=LlamaRotaryEmbedding)\n",
       "          )\n",
       "          (mlp): LlamaMLP(\n",
       "            original_name=LlamaMLP\n",
       "            (gate_proj): Linear(original_name=Linear)\n",
       "            (up_proj): Linear(original_name=Linear)\n",
       "            (down_proj): Linear(original_name=Linear)\n",
       "            (act_fn): SiLUActivation(original_name=SiLUActivation)\n",
       "          )\n",
       "          (input_layernorm): LlamaRMSNorm(original_name=LlamaRMSNorm)\n",
       "          (post_attention_layernorm): LlamaRMSNorm(original_name=LlamaRMSNorm)\n",
       "        )\n",
       "        (27): LlamaDecoderLayer(\n",
       "          original_name=LlamaDecoderLayer\n",
       "          (self_attn): LlamaAttention(\n",
       "            original_name=LlamaAttention\n",
       "            (q_proj): Linear(original_name=Linear)\n",
       "            (k_proj): Linear(original_name=Linear)\n",
       "            (v_proj): Linear(original_name=Linear)\n",
       "            (o_proj): Linear(original_name=Linear)\n",
       "            (rotary_emb): LlamaRotaryEmbedding(original_name=LlamaRotaryEmbedding)\n",
       "          )\n",
       "          (mlp): LlamaMLP(\n",
       "            original_name=LlamaMLP\n",
       "            (gate_proj): Linear(original_name=Linear)\n",
       "            (up_proj): Linear(original_name=Linear)\n",
       "            (down_proj): Linear(original_name=Linear)\n",
       "            (act_fn): SiLUActivation(original_name=SiLUActivation)\n",
       "          )\n",
       "          (input_layernorm): LlamaRMSNorm(original_name=LlamaRMSNorm)\n",
       "          (post_attention_layernorm): LlamaRMSNorm(original_name=LlamaRMSNorm)\n",
       "        )\n",
       "        (28): LlamaDecoderLayer(\n",
       "          original_name=LlamaDecoderLayer\n",
       "          (self_attn): LlamaAttention(\n",
       "            original_name=LlamaAttention\n",
       "            (q_proj): Linear(original_name=Linear)\n",
       "            (k_proj): Linear(original_name=Linear)\n",
       "            (v_proj): Linear(original_name=Linear)\n",
       "            (o_proj): Linear(original_name=Linear)\n",
       "            (rotary_emb): LlamaRotaryEmbedding(original_name=LlamaRotaryEmbedding)\n",
       "          )\n",
       "          (mlp): LlamaMLP(\n",
       "            original_name=LlamaMLP\n",
       "            (gate_proj): Linear(original_name=Linear)\n",
       "            (up_proj): Linear(original_name=Linear)\n",
       "            (down_proj): Linear(original_name=Linear)\n",
       "            (act_fn): SiLUActivation(original_name=SiLUActivation)\n",
       "          )\n",
       "          (input_layernorm): LlamaRMSNorm(original_name=LlamaRMSNorm)\n",
       "          (post_attention_layernorm): LlamaRMSNorm(original_name=LlamaRMSNorm)\n",
       "        )\n",
       "        (29): LlamaDecoderLayer(\n",
       "          original_name=LlamaDecoderLayer\n",
       "          (self_attn): LlamaAttention(\n",
       "            original_name=LlamaAttention\n",
       "            (q_proj): Linear(original_name=Linear)\n",
       "            (k_proj): Linear(original_name=Linear)\n",
       "            (v_proj): Linear(original_name=Linear)\n",
       "            (o_proj): Linear(original_name=Linear)\n",
       "            (rotary_emb): LlamaRotaryEmbedding(original_name=LlamaRotaryEmbedding)\n",
       "          )\n",
       "          (mlp): LlamaMLP(\n",
       "            original_name=LlamaMLP\n",
       "            (gate_proj): Linear(original_name=Linear)\n",
       "            (up_proj): Linear(original_name=Linear)\n",
       "            (down_proj): Linear(original_name=Linear)\n",
       "            (act_fn): SiLUActivation(original_name=SiLUActivation)\n",
       "          )\n",
       "          (input_layernorm): LlamaRMSNorm(original_name=LlamaRMSNorm)\n",
       "          (post_attention_layernorm): LlamaRMSNorm(original_name=LlamaRMSNorm)\n",
       "        )\n",
       "        (30): LlamaDecoderLayer(\n",
       "          original_name=LlamaDecoderLayer\n",
       "          (self_attn): LlamaAttention(\n",
       "            original_name=LlamaAttention\n",
       "            (q_proj): Linear(original_name=Linear)\n",
       "            (k_proj): Linear(original_name=Linear)\n",
       "            (v_proj): Linear(original_name=Linear)\n",
       "            (o_proj): Linear(original_name=Linear)\n",
       "            (rotary_emb): LlamaRotaryEmbedding(original_name=LlamaRotaryEmbedding)\n",
       "          )\n",
       "          (mlp): LlamaMLP(\n",
       "            original_name=LlamaMLP\n",
       "            (gate_proj): Linear(original_name=Linear)\n",
       "            (up_proj): Linear(original_name=Linear)\n",
       "            (down_proj): Linear(original_name=Linear)\n",
       "            (act_fn): SiLUActivation(original_name=SiLUActivation)\n",
       "          )\n",
       "          (input_layernorm): LlamaRMSNorm(original_name=LlamaRMSNorm)\n",
       "          (post_attention_layernorm): LlamaRMSNorm(original_name=LlamaRMSNorm)\n",
       "        )\n",
       "        (31): LlamaDecoderLayer(\n",
       "          original_name=LlamaDecoderLayer\n",
       "          (self_attn): LlamaAttention(\n",
       "            original_name=LlamaAttention\n",
       "            (q_proj): Linear(original_name=Linear)\n",
       "            (k_proj): Linear(original_name=Linear)\n",
       "            (v_proj): Linear(original_name=Linear)\n",
       "            (o_proj): Linear(original_name=Linear)\n",
       "            (rotary_emb): LlamaRotaryEmbedding(original_name=LlamaRotaryEmbedding)\n",
       "          )\n",
       "          (mlp): LlamaMLP(\n",
       "            original_name=LlamaMLP\n",
       "            (gate_proj): Linear(original_name=Linear)\n",
       "            (up_proj): Linear(original_name=Linear)\n",
       "            (down_proj): Linear(original_name=Linear)\n",
       "            (act_fn): SiLUActivation(original_name=SiLUActivation)\n",
       "          )\n",
       "          (input_layernorm): LlamaRMSNorm(original_name=LlamaRMSNorm)\n",
       "          (post_attention_layernorm): LlamaRMSNorm(original_name=LlamaRMSNorm)\n",
       "        )\n",
       "      )\n",
       "      (norm): LlamaRMSNorm(original_name=LlamaRMSNorm)\n",
       "    )\n",
       "    (lm_head): Linear(original_name=Linear)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traced_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "570.3120026789838\n"
     ]
    }
   ],
   "source": [
    "def timer(f, *args):\n",
    "    torch.cuda.synchronize()  # 确保之前所有的GPU操作完成\n",
    "    start = perf_counter()\n",
    "    f(*args)\n",
    "    torch.cuda.synchronize()  # 确保当前所有的GPU操作完成\n",
    "    return 1000 * (perf_counter() - start)\n",
    "\n",
    "# 测量traced_model的执行时间\n",
    "times = [timer(traced_model, inputs.input_ids) for _ in range(100)]\n",
    "print(np.mean(times))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(text, return_tensors='pt').to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average inference time over 100 runs: 55.32 ms\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def timer(f, *args):\n",
    "    torch.cuda.synchronize()  # 确保之前所有的GPU操作完成\n",
    "    start = perf_counter()\n",
    "    f(*args)\n",
    "    torch.cuda.synchronize()  # 确保当前所有的GPU操作完成\n",
    "    return 1000 * (perf_counter() - start)\n",
    "\n",
    "# 定义包装函数以简化模型输出\n",
    "def model_forward(input_ids):\n",
    "    outputs = model(input_ids)\n",
    "    return outputs.logits\n",
    "\n",
    "# 测量模型的执行时间\n",
    "times = []\n",
    "for _ in range(100):\n",
    "    elapsed_time = timer(model_forward, inputs.input_ids)\n",
    "    times.append(elapsed_time)\n",
    "\n",
    "# 计算平均时间\n",
    "average_time = np.mean(times)\n",
    "print(f\"Average inference time over 100 runs: {average_time:.2f} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06358981132507324\n"
     ]
    }
   ],
   "source": [
    "time1=time.time()\n",
    "timer(model_forward, inputs.input_ids)\n",
    "time2 = time.time()\n",
    "time_elapsed = time2-time1\n",
    "print(time_elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    " \n",
    "class Model(torch.nn.Module): \n",
    "    def __init__(self, n): \n",
    "        super().__init__() \n",
    "        self.n = n \n",
    "        self.conv = torch.nn.Conv2d(3, 3, 3) \n",
    " \n",
    "    def forward(self, x): \n",
    "        for i in range(self.n): \n",
    "            x = self.conv(x) \n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [Model(2), Model(3)] \n",
    "model_names = ['model_2', 'model_3'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/onnx/utils.py:847: UserWarning: no signature found for <torch.ScriptMethod object at 0x7f78a2d5a520>, skipping _decide_input_format\n",
      "  warnings.warn(f\"{e}, skipping _decide_input_format\")\n"
     ]
    }
   ],
   "source": [
    "for model, model_name in zip(models, model_names): \n",
    "    dummy_input = torch.rand(1, 3, 10, 10) \n",
    "    dummy_output = model(dummy_input) \n",
    "    model_trace = torch.jit.trace(model, dummy_input) \n",
    "    model_script = torch.jit.script(model) \n",
    "    # 跟踪法与直接 torch.onnx.export(model, ...)等价 \n",
    "    torch.onnx.export(model_trace, dummy_input, f'/home/Graph_module/oonx/{model_name}_trace.onnx') \n",
    "    # 记录法必须先调用 torch.jit.sciprt \n",
    "    torch.onnx.export(model_script, dummy_input, f'/home/Graph_module/oonx/{model_name}_script.onnx') "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
