{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import numpy as np\n",
    "import torch\n",
    "from time import perf_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timer(f,*args):   \n",
    "    torch.cuda.synchronize() \n",
    "    start = perf_counter()\n",
    "\n",
    "    f(*args)\n",
    "    torch.cuda.synchronize() \n",
    "    return (1000 * (perf_counter() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载bert model\n",
    "native_model = BertModel.from_pretrained(\"/dataset/crosspipe/bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_model = BertModel.from_pretrained(\"/dataset/crosspipe/bert-base-uncased\", torchscript=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "script_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_tokenizer = BertTokenizer.from_pretrained('/dataset/crosspipe/bert-base-uncased', torchscript=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"[CLS] Who was Jim Henson ? [SEP] Jim Henson was a puppeteer [SEP]\"\n",
    "tokenized_text = script_tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Masking one of the input tokens\n",
    "masked_index = 8\n",
    "tokenized_text[masked_index] = '[MASK]'\n",
    "indexed_tokens = script_tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "segments_ids = [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]\n",
    "# Creating a dummy input\n",
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "segments_tensors = torch.tensor([segments_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[-2.5689e-01, -7.3601e-03, -8.9146e-02,  ..., -1.3546e-01,\n",
       "           2.3597e-01,  2.4208e-01],\n",
       "         [-5.8262e-01,  3.1923e-01, -2.8020e-01,  ...,  1.0413e-01,\n",
       "           1.7953e-01, -4.7086e-01],\n",
       "         [-3.0671e-01, -2.3213e-01, -1.5938e-01,  ...,  7.0993e-02,\n",
       "           1.4761e-01,  2.7529e-01],\n",
       "         ...,\n",
       "         [ 2.0549e-01, -1.6317e-02, -7.1208e-05,  ..., -1.3032e-01,\n",
       "           6.1008e-01,  4.2999e-01],\n",
       "         [-4.9530e-01, -4.6195e-01, -2.9027e-01,  ...,  6.3559e-01,\n",
       "           6.2100e-01,  1.0318e-01],\n",
       "         [ 8.2051e-01,  1.8250e-01, -1.1302e-01,  ...,  1.5103e-01,\n",
       "          -7.6513e-01, -1.9481e-02]]], grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[-4.9859e-01, -1.6913e-01,  8.3044e-01,  7.2490e-02, -4.8807e-01,\n",
       "         -9.1259e-02,  5.1964e-01,  1.2615e-01,  7.3988e-01, -9.9609e-01,\n",
       "          3.7945e-01, -5.8106e-01,  9.5275e-01, -6.8154e-01,  7.0220e-01,\n",
       "         -2.4374e-01,  9.2702e-02, -3.1205e-01,  2.3801e-01, -3.3289e-01,\n",
       "          2.2093e-01, -3.1050e-01,  7.1334e-01,  6.8512e-02,  1.5540e-01,\n",
       "         -7.6150e-01, -2.8993e-01,  7.5329e-01,  8.3334e-01,  5.7146e-01,\n",
       "         -3.3142e-01,  1.4806e-01, -9.5029e-01, -9.6337e-02,  7.7407e-01,\n",
       "         -9.3747e-01, -3.0848e-02, -4.7886e-01,  4.4815e-02,  1.1357e-02,\n",
       "         -6.1463e-01,  2.0118e-01,  9.0852e-01, -6.6917e-01, -3.0572e-01,\n",
       "         -2.4733e-01, -6.9975e-01,  9.1704e-02, -6.5932e-01, -8.4444e-01,\n",
       "         -7.1574e-01, -8.7206e-01,  5.5364e-02,  1.0259e-01,  2.0758e-01,\n",
       "          4.1869e-01, -2.2090e-01,  6.6651e-02,  3.9337e-02, -3.4385e-01,\n",
       "         -4.0217e-01,  3.1609e-02,  6.8003e-01, -7.3265e-01, -7.0735e-01,\n",
       "         -8.4914e-01, -3.4450e-02, -7.7896e-02,  1.1542e-01, -5.8604e-02,\n",
       "          5.3955e-01,  7.7160e-02,  4.1768e-01, -4.7927e-01, -7.7228e-01,\n",
       "          1.0024e-01, -1.9495e-01,  9.0035e-01,  2.1756e-01, -9.3141e-01,\n",
       "         -6.6439e-01, -6.5286e-01,  1.0849e-01,  8.2425e-01, -8.0041e-01,\n",
       "         -8.0010e-01,  1.9082e-01, -8.2846e-03, -9.5559e-01,  1.0529e-01,\n",
       "          4.8708e-02, -2.7277e-02, -7.0070e-01,  1.2284e-01,  1.0367e-01,\n",
       "          1.1624e-01, -3.6920e-02,  6.1928e-01,  3.4673e-02,  2.5419e-01,\n",
       "         -1.0259e-01,  5.9034e-03,  2.2802e-01, -1.5938e-01, -9.4064e-02,\n",
       "         -1.3483e-01,  9.1638e-02, -2.5277e-01, -4.0633e-01,  3.5905e-01,\n",
       "          1.7583e-01, -5.4669e-02, -3.7656e-02, -8.2613e-01,  3.9060e-01,\n",
       "         -7.5958e-02, -9.2011e-01, -1.5846e-01, -9.5379e-01,  4.7328e-01,\n",
       "          2.2438e-01, -4.0782e-02,  8.4882e-01,  7.4372e-01,  6.8335e-02,\n",
       "          1.2067e-01,  8.3558e-01, -9.3571e-01,  2.5638e-01,  2.7365e-01,\n",
       "          5.1111e-01,  2.4958e-03, -9.1422e-01, -8.5609e-01,  2.6730e-01,\n",
       "          8.2105e-01, -1.9207e-02,  9.0641e-01,  3.1728e-02,  8.1417e-01,\n",
       "          5.3713e-01,  1.5256e-01, -6.5996e-01, -2.9205e-01, -2.6196e-01,\n",
       "         -6.0163e-02, -1.6912e-01,  1.9751e-01,  4.2851e-01, -5.6606e-01,\n",
       "          3.2405e-01, -1.4614e-01,  7.3622e-01, -7.3622e-01, -3.0769e-01,\n",
       "          7.5544e-01,  5.2388e-01,  8.3670e-01,  7.7317e-01,  2.1110e-02,\n",
       "         -1.0186e-01,  5.3786e-01, -1.1288e-01,  1.2010e-01,  2.8660e-01,\n",
       "          1.5836e-01, -5.5910e-01,  1.3677e-01, -6.5858e-01,  4.6724e-01,\n",
       "          1.8676e-01, -7.8809e-04,  8.0405e-01, -9.3026e-01, -6.9134e-02,\n",
       "          2.2026e-01,  9.3376e-01,  4.8834e-01,  6.0626e-02, -5.2468e-01,\n",
       "         -6.7205e-02, -2.8043e-01, -8.0448e-01,  9.1711e-01,  3.1231e-02,\n",
       "          1.4967e-01,  5.4755e-01, -5.2422e-01, -6.1328e-01, -6.5674e-01,\n",
       "          5.5004e-01,  3.5562e-01, -6.2681e-01,  1.7565e-01, -2.6099e-01,\n",
       "         -1.2162e-01,  5.5805e-01,  1.7059e-01, -7.8489e-02, -2.6815e-01,\n",
       "          1.2040e-01,  7.8531e-01,  6.7986e-01,  4.0405e-01, -7.0169e-01,\n",
       "          1.0016e-01, -6.9205e-01,  3.5772e-02,  7.4413e-02,  1.4686e-01,\n",
       "         -4.7862e-02,  9.5771e-01,  2.1660e-01,  6.9759e-04, -6.7157e-01,\n",
       "         -9.2958e-01,  1.2180e-02, -6.3569e-01,  1.3246e-01, -3.6443e-01,\n",
       "         -1.9593e-01,  6.1201e-01, -5.9815e-01,  7.9575e-02, -7.0562e-01,\n",
       "         -5.6153e-01,  9.3639e-02, -1.2182e-01,  1.3971e-01, -5.4726e-02,\n",
       "          3.1027e-01, -7.9235e-01, -2.9843e-01,  3.6523e-01,  7.2965e-01,\n",
       "          8.2288e-01, -4.8346e-01,  3.6060e-01, -1.5701e-02,  6.6516e-01,\n",
       "         -3.1768e-01,  8.6993e-01, -6.7448e-01, -7.0251e-02, -7.9001e-01,\n",
       "          5.6488e-01, -6.6356e-01,  6.4933e-01, -4.8079e-03, -8.2526e-01,\n",
       "         -6.7866e-01,  1.6267e-01,  5.3347e-02,  8.8357e-01, -7.1264e-02,\n",
       "          8.2370e-01, -5.7167e-01, -8.5698e-01, -1.2295e-01,  2.3306e-01,\n",
       "         -9.4883e-01, -6.4056e-01,  9.9413e-02, -6.6189e-01, -1.6533e-01,\n",
       "          7.5132e-03, -8.3298e-01,  4.5723e-01,  8.1019e-02,  8.3341e-01,\n",
       "          2.2406e-01, -5.3090e-01,  1.4762e-01, -7.6026e-01, -1.6274e-01,\n",
       "          1.3930e-01,  8.2007e-01, -9.1361e-02, -8.5741e-01,  2.0810e-01,\n",
       "          2.9116e-01,  1.5543e-01,  8.5719e-01,  8.7265e-01,  6.3798e-01,\n",
       "          9.2237e-01,  7.0844e-01,  4.0798e-01,  3.7541e-01,  8.2557e-02,\n",
       "          9.9737e-01,  6.1868e-01, -7.9912e-01, -8.2941e-01, -2.8434e-01,\n",
       "          1.2858e-01, -9.2797e-01, -6.3592e-02,  4.6552e-02, -8.1194e-01,\n",
       "         -7.1708e-01,  9.1365e-01,  7.2862e-01, -8.9509e-01,  7.3806e-01,\n",
       "          7.2612e-01, -2.3452e-01, -6.6035e-01,  6.3779e-02,  9.3092e-01,\n",
       "          1.6669e-02,  3.3494e-01, -1.8453e-02,  1.5112e-01,  4.9735e-01,\n",
       "         -5.7902e-01,  6.5534e-01,  6.6720e-01, -6.5396e-01, -8.4177e-02,\n",
       "         -1.6271e-01, -7.9912e-01, -3.1524e-01,  1.7048e-03, -3.4439e-01,\n",
       "         -8.4678e-01, -2.0301e-02, -5.9327e-01,  1.5047e-01, -4.0946e-03,\n",
       "          1.0241e-03, -5.2182e-01,  4.0745e-02, -8.2124e-01,  1.3655e-01,\n",
       "          3.0421e-01, -7.6729e-01, -1.4364e-01,  3.7359e-01, -5.8641e-01,\n",
       "          6.7145e-01, -8.4058e-01,  8.8928e-01, -4.8595e-02, -7.0527e-01,\n",
       "          9.0982e-01, -1.3674e-01, -6.2551e-01, -1.2153e-01, -5.7317e-02,\n",
       "          1.6933e-01,  8.5137e-01, -3.0268e-01, -9.2407e-01, -1.4311e-01,\n",
       "         -2.2600e-01, -4.9942e-02,  7.3210e-02,  9.7865e-01,  3.2855e-02,\n",
       "          6.2553e-01,  6.3310e-01,  9.3339e-01, -9.5555e-01, -6.6198e-01,\n",
       "         -6.5063e-01, -8.4128e-01,  8.7723e-01,  7.9893e-01,  1.4459e-01,\n",
       "         -1.9865e-01, -6.9513e-02,  6.3042e-01, -1.2870e-02, -7.2304e-01,\n",
       "          2.7195e-01,  2.2727e-01, -2.9499e-02,  7.1309e-01, -4.8478e-01,\n",
       "         -1.1718e-01,  2.0216e-01,  4.6508e-01,  4.6507e-01, -7.6713e-01,\n",
       "          2.0409e-01, -5.9568e-03, -8.2280e-02, -8.4956e-02, -1.4858e-01,\n",
       "         -8.5327e-01, -3.6672e-01,  8.7005e-01,  1.9555e-01, -6.6828e-01,\n",
       "          1.5964e-01, -1.6004e-02, -3.9608e-01,  1.0805e-01,  9.5431e-02,\n",
       "         -8.3893e-02, -5.9011e-01, -6.5418e-01, -5.9851e-01, -9.5626e-01,\n",
       "          2.0579e-01,  2.3757e-02, -3.0544e-02,  5.7845e-01,  1.2862e-01,\n",
       "         -8.2294e-03, -2.6664e-01, -6.8082e-01, -9.6271e-02,  6.9126e-02,\n",
       "         -8.6602e-01,  8.7522e-01, -1.0679e-01,  1.2648e-01,  3.2427e-01,\n",
       "          7.7688e-01, -9.3565e-02, -3.4144e-01, -4.7951e-02, -7.3323e-01,\n",
       "          1.4223e-01, -8.2471e-01,  8.3261e-01, -8.1426e-01,  7.6197e-02,\n",
       "         -5.7766e-02, -4.5483e-01,  9.0927e-01, -1.8027e-01,  2.0544e-01,\n",
       "         -1.8068e-01,  4.2399e-01,  3.3363e-01, -2.0288e-01, -2.2413e-01,\n",
       "          3.6761e-02,  8.0875e-01, -1.0997e-03,  3.5741e-02, -9.0296e-01,\n",
       "         -7.9219e-01, -5.2211e-01, -6.7589e-01, -9.5255e-01,  7.0411e-01,\n",
       "          2.6826e-01,  4.0883e-02,  5.4808e-01, -2.2253e-01, -2.8980e-01,\n",
       "         -8.3246e-02,  1.0494e-01, -8.2818e-01,  7.8696e-01, -1.0552e-01,\n",
       "          2.0068e-01, -1.1545e-01,  2.2196e-01, -8.5198e-01,  8.6098e-01,\n",
       "          7.5903e-01,  2.5622e-01, -8.8535e-03, -4.7662e-01,  3.9720e-01,\n",
       "         -3.4306e-01,  8.0380e-01, -7.8320e-02,  8.9613e-01, -1.4736e-01,\n",
       "         -7.5046e-01,  3.7599e-01,  2.0120e-01,  6.4580e-02,  4.4332e-02,\n",
       "         -8.3205e-01,  3.1302e-03,  7.8753e-01,  7.9850e-01, -4.4507e-01,\n",
       "         -5.6260e-03, -3.3181e-02, -5.8861e-01, -7.9499e-01,  4.4164e-01,\n",
       "         -3.1938e-01, -3.7808e-02,  6.6738e-02,  1.9534e-02,  8.4942e-01,\n",
       "          5.3234e-03,  9.4798e-02, -1.2333e-01,  8.5869e-02, -1.0280e-01,\n",
       "         -1.2946e-01,  8.5376e-01,  1.8801e-01, -3.2741e-01, -9.6287e-01,\n",
       "          6.1638e-01, -6.9420e-01,  4.4449e-01,  6.0681e-01, -6.6629e-01,\n",
       "          5.0973e-02,  3.4253e-02, -1.8121e-01,  2.2566e-01, -1.4444e-02,\n",
       "         -1.3434e-01,  4.3035e-02,  1.1194e-01,  9.0154e-01, -2.0848e-01,\n",
       "         -8.8703e-01, -3.1255e-01,  6.9419e-02, -8.3545e-01, -2.6752e-01,\n",
       "         -2.3816e-01,  2.7348e-03, -1.5042e-01,  4.7942e-01,  3.3011e-01,\n",
       "         -1.6503e-01, -9.0385e-01, -6.1607e-02, -1.1797e-01,  9.0846e-01,\n",
       "          2.6326e-02, -1.3698e-01, -6.5338e-01, -7.4803e-01, -4.6147e-01,\n",
       "          8.1519e-01, -8.4302e-01,  9.1290e-01, -8.3432e-01,  3.5687e-03,\n",
       "          7.8573e-01,  2.6059e-01, -7.7205e-01, -3.1327e-03, -6.6097e-02,\n",
       "          5.1946e-02,  3.9750e-01,  1.4423e-01, -8.5038e-01, -1.8162e-02,\n",
       "         -1.0716e-02,  1.1313e-01,  8.5654e-04,  3.9124e-01,  4.4546e-01,\n",
       "          9.0402e-02, -1.6985e-01, -3.3805e-01,  6.1812e-02,  2.2982e-01,\n",
       "          2.3385e-01, -1.2379e-01,  4.5419e-02,  5.1026e-02, -7.1878e-04,\n",
       "         -6.6718e-01, -1.3307e-01,  7.8950e-02, -2.0888e-02,  3.0005e-01,\n",
       "         -9.1189e-01, -6.5355e-01, -7.6801e-01, -9.3508e-02,  5.3021e-01,\n",
       "          2.1915e-02, -4.8867e-01, -3.5073e-01,  8.0068e-01,  8.4983e-01,\n",
       "          3.5458e-01,  2.0712e-02,  5.9287e-01, -3.4995e-01, -7.2958e-02,\n",
       "          3.9693e-02,  7.5798e-02,  6.1057e-01,  4.2597e-01, -2.0743e-02,\n",
       "          9.1923e-01, -5.0431e-03,  5.8217e-02, -6.0209e-01,  1.3403e-01,\n",
       "         -7.2890e-02,  6.6255e-01, -4.6876e-01, -8.6120e-01,  1.0894e-02,\n",
       "          1.8973e-02, -4.4184e-01,  2.5289e-02, -1.1211e-01, -3.0922e-01,\n",
       "          6.5002e-01,  7.4250e-01,  5.5601e-01, -1.8821e-01,  1.3612e-01,\n",
       "         -9.7375e-02, -4.0479e-02, -2.4442e-02, -7.8018e-01,  9.5698e-01,\n",
       "          1.7359e-01,  4.7505e-01,  3.3328e-01,  2.2359e-01,  8.7758e-01,\n",
       "          1.3941e-02,  3.2011e-01, -1.0929e-02,  7.9241e-01,  6.8529e-02,\n",
       "         -7.1604e-01,  5.0206e-01, -9.1000e-01, -3.8872e-02, -7.6048e-01,\n",
       "          1.7364e-01, -1.7590e-01,  5.9545e-01, -1.0509e-01,  8.5677e-01,\n",
       "          8.0183e-01, -1.7132e-02,  4.1293e-01,  8.1418e-01,  1.5888e-01,\n",
       "         -7.5477e-01, -9.5126e-01, -9.6628e-01, -7.2032e-02, -2.3228e-01,\n",
       "          4.7854e-02,  1.7156e-01, -2.8391e-02,  1.4297e-01,  3.4419e-02,\n",
       "         -8.3534e-01,  7.9760e-01,  1.2672e-01, -8.1043e-01,  8.8525e-01,\n",
       "         -3.8777e-01,  6.0805e-02,  2.3121e-01, -9.4670e-01, -5.4725e-01,\n",
       "         -1.0510e-01, -1.3351e-01,  3.5532e-01,  1.4217e-01,  5.4140e-01,\n",
       "          1.9414e-02, -3.2170e-01, -6.5439e-02,  7.8295e-01, -5.2289e-01,\n",
       "         -9.5798e-01,  2.8194e-01,  6.3611e-01, -5.6436e-01,  8.3533e-01,\n",
       "         -6.4246e-01, -1.0839e-01,  7.5424e-01,  7.2921e-01,  5.5158e-01,\n",
       "          4.3062e-01,  2.0447e-01,  1.1434e-01,  3.8707e-01,  6.9555e-01,\n",
       "          7.4856e-01,  9.5958e-01,  6.6706e-01,  2.7684e-01,  7.0061e-01,\n",
       "          2.4441e-02,  4.2779e-01, -8.4437e-01, -5.7101e-02, -4.4115e-01,\n",
       "          2.1374e-01,  1.0470e-01, -4.6251e-02, -6.4018e-01,  4.0994e-01,\n",
       "          7.6299e-02,  1.8739e-01, -8.4008e-02,  2.8767e-01, -2.4358e-01,\n",
       "          1.1020e-02, -5.1402e-01,  4.9222e-02,  2.6653e-01, -5.8301e-02,\n",
       "          7.9280e-01, -2.4956e-01, -7.8913e-03, -1.1232e-01,  5.1617e-02,\n",
       "          7.6497e-01, -8.3232e-01,  5.1334e-01,  5.9828e-02,  8.3035e-01,\n",
       "         -5.8532e-01, -2.4030e-01,  6.1904e-01, -3.5076e-01, -1.3420e-01,\n",
       "         -8.9157e-02, -4.9536e-01,  4.9910e-01, -4.5499e-02, -1.6105e-01,\n",
       "         -3.7617e-02,  3.7182e-01,  1.6940e-01, -1.1236e-01,  5.3460e-01,\n",
       "          7.2991e-01,  1.8875e-01,  4.1904e-02,  1.7980e-01,  6.0191e-02,\n",
       "         -7.9329e-01,  1.4009e-01,  6.1139e-01, -6.1764e-01,  3.4262e-01,\n",
       "         -6.7680e-01,  4.8117e-01, -7.8062e-01,  4.0878e-02, -3.7002e-01,\n",
       "         -6.8798e-01, -3.8813e-01,  1.6272e-01,  1.2418e-01,  7.1285e-01,\n",
       "         -7.0520e-01,  6.1055e-01,  3.9583e-01,  7.1228e-01,  1.4088e-01,\n",
       "          5.9301e-01, -3.6854e-01,  5.7186e-01]], grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "native_model(tokens_tensor,segments_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28.176284969667904"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "native_model.eval()\n",
    "np.mean([timer(native_model,tokens_tensor,segments_tensors) for _ in range(100)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "native_model = native_model.cuda()\n",
    "native_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.522816987941042"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_tensor_gpu = tokens_tensor.cuda()\n",
    "segments_tensors_gpu = segments_tensors.cuda()\n",
    "np.mean([timer(native_model,tokens_tensor_gpu,segments_tensors_gpu) for _ in range(100)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_tensor.cpu()\n",
    "segments_tensors.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19.31729846925009"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traced_model = torch.jit.trace(script_model, [tokens_tensor, segments_tensors])\n",
    "# 因模型的trace时，已经包含了.eval()的行为，因此不必再去显式调用model.eval()\n",
    "np.mean([timer(traced_model,tokens_tensor,segments_tensors) for _ in range(100)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_model.cuda()\n",
    "script_model.eval()\n",
    "tokens_tensor_gpu = tokens_tensor.cuda()\n",
    "segments_tensors_gpu = segments_tensors.cuda()\n",
    "traced_model = torch.jit.trace(script_model, [tokens_tensor_gpu, segments_tensors_gpu])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "default_program(24): error: extra text after expected end of number\n          aten_mul[(long long)(threadIdx.x) + 512ll * (long long)(blockIdx.x)] = v * -3.402823466385289e+38.f;\n                                                                                                           ^\n\ndefault_program(28): error: extra text after expected end of number\n      aten_add[(long long)(threadIdx.x) + 512ll * (long long)(blockIdx.x)] = v_1 / 8.f + v_2 * -3.402823466385289e+38.f;\n                                                                                                                     ^\n\n2 errors detected in the compilation of \"default_program\".\n\nnvrtc compilation failed: \n\n#define NAN __int_as_float(0x7fffffff)\n#define POS_INFINITY __int_as_float(0x7f800000)\n#define NEG_INFINITY __int_as_float(0xff800000)\n\n\ntemplate<typename T>\n__device__ T maximum(T a, T b) {\n  return isnan(a) ? a : (a > b ? a : b);\n}\n\ntemplate<typename T>\n__device__ T minimum(T a, T b) {\n  return isnan(a) ? a : (a < b ? a : b);\n}\n\nextern \"C\" __global__\nvoid fused_mul_div_add(float* tattention_scores_1, float* tv_, float* aten_add, float* aten_mul) {\n{\nif (blockIdx.x<1ll ? 1 : 0) {\nif ((long long)(threadIdx.x) + 512ll * (long long)(blockIdx.x)<14ll ? 1 : 0) {\nif (blockIdx.x<1ll ? 1 : 0) {\n        float v = __ldg(tv_ + (long long)(threadIdx.x) + 512ll * (long long)(blockIdx.x));\n        aten_mul[(long long)(threadIdx.x) + 512ll * (long long)(blockIdx.x)] = v * -3.402823466385289e+38.f;\n      }    }  }if ((long long)(threadIdx.x) + 512ll * (long long)(blockIdx.x)<2352ll ? 1 : 0) {\n    float v_1 = __ldg(tattention_scores_1 + (long long)(threadIdx.x) + 512ll * (long long)(blockIdx.x));\n    float v_2 = __ldg(tv_ + ((long long)(threadIdx.x) + 512ll * (long long)(blockIdx.x)) % 14ll);\n    aten_add[(long long)(threadIdx.x) + 512ll * (long long)(blockIdx.x)] = v_1 / 8.f + v_2 * -3.402823466385289e+38.f;\n  }}\n}\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m np\u001b[38;5;241m.\u001b[39mmean([timer(traced_model,tokens_tensor_gpu,segments_tensors_gpu) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m100\u001b[39m)])\n",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0m np\u001b[38;5;241m.\u001b[39mmean([\u001b[43mtimer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraced_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtokens_tensor_gpu\u001b[49m\u001b[43m,\u001b[49m\u001b[43msegments_tensors_gpu\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m100\u001b[39m)])\n",
      "Cell \u001b[0;32mIn[2], line 5\u001b[0m, in \u001b[0;36mtimer\u001b[0;34m(f, *args)\u001b[0m\n\u001b[1;32m      2\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39msynchronize() \n\u001b[1;32m      3\u001b[0m start \u001b[38;5;241m=\u001b[39m perf_counter()\n\u001b[0;32m----> 5\u001b[0m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39msynchronize() \n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;241m1000\u001b[39m \u001b[38;5;241m*\u001b[39m (perf_counter() \u001b[38;5;241m-\u001b[39m start))\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: default_program(24): error: extra text after expected end of number\n          aten_mul[(long long)(threadIdx.x) + 512ll * (long long)(blockIdx.x)] = v * -3.402823466385289e+38.f;\n                                                                                                           ^\n\ndefault_program(28): error: extra text after expected end of number\n      aten_add[(long long)(threadIdx.x) + 512ll * (long long)(blockIdx.x)] = v_1 / 8.f + v_2 * -3.402823466385289e+38.f;\n                                                                                                                     ^\n\n2 errors detected in the compilation of \"default_program\".\n\nnvrtc compilation failed: \n\n#define NAN __int_as_float(0x7fffffff)\n#define POS_INFINITY __int_as_float(0x7f800000)\n#define NEG_INFINITY __int_as_float(0xff800000)\n\n\ntemplate<typename T>\n__device__ T maximum(T a, T b) {\n  return isnan(a) ? a : (a > b ? a : b);\n}\n\ntemplate<typename T>\n__device__ T minimum(T a, T b) {\n  return isnan(a) ? a : (a < b ? a : b);\n}\n\nextern \"C\" __global__\nvoid fused_mul_div_add(float* tattention_scores_1, float* tv_, float* aten_add, float* aten_mul) {\n{\nif (blockIdx.x<1ll ? 1 : 0) {\nif ((long long)(threadIdx.x) + 512ll * (long long)(blockIdx.x)<14ll ? 1 : 0) {\nif (blockIdx.x<1ll ? 1 : 0) {\n        float v = __ldg(tv_ + (long long)(threadIdx.x) + 512ll * (long long)(blockIdx.x));\n        aten_mul[(long long)(threadIdx.x) + 512ll * (long long)(blockIdx.x)] = v * -3.402823466385289e+38.f;\n      }    }  }if ((long long)(threadIdx.x) + 512ll * (long long)(blockIdx.x)<2352ll ? 1 : 0) {\n    float v_1 = __ldg(tattention_scores_1 + (long long)(threadIdx.x) + 512ll * (long long)(blockIdx.x));\n    float v_2 = __ldg(tv_ + ((long long)(threadIdx.x) + 512ll * (long long)(blockIdx.x)) % 14ll);\n    aten_add[(long long)(threadIdx.x) + 512ll * (long long)(blockIdx.x)] = v_1 / 8.f + v_2 * -3.402823466385289e+38.f;\n  }}\n}\n"
     ]
    }
   ],
   "source": [
    "np.mean([timer(traced_model,tokens_tensor_gpu,segments_tensors_gpu) for _ in range(100)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average execution time for native model on GPU: 8.83 ms\n",
      "Tracing failed with error: default_program(24): error: extra text after expected end of number\n",
      "          aten_mul[(long long)(threadIdx.x) + 512ll * (long long)(blockIdx.x)] = v * -3.402823466385289e+38.f;\n",
      "                                                                                                           ^\n",
      "\n",
      "default_program(28): error: extra text after expected end of number\n",
      "      aten_add[(long long)(threadIdx.x) + 512ll * (long long)(blockIdx.x)] = v_1 / 8.f + v_2 * -3.402823466385289e+38.f;\n",
      "                                                                                                                     ^\n",
      "\n",
      "2 errors detected in the compilation of \"default_program\".\n",
      "\n",
      "nvrtc compilation failed: \n",
      "\n",
      "#define NAN __int_as_float(0x7fffffff)\n",
      "#define POS_INFINITY __int_as_float(0x7f800000)\n",
      "#define NEG_INFINITY __int_as_float(0xff800000)\n",
      "\n",
      "\n",
      "template<typename T>\n",
      "__device__ T maximum(T a, T b) {\n",
      "  return isnan(a) ? a : (a > b ? a : b);\n",
      "}\n",
      "\n",
      "template<typename T>\n",
      "__device__ T minimum(T a, T b) {\n",
      "  return isnan(a) ? a : (a < b ? a : b);\n",
      "}\n",
      "\n",
      "extern \"C\" __global__\n",
      "void fused_mul_div_add(float* tattention_scores_1, float* tv_, float* aten_add, float* aten_mul) {\n",
      "{\n",
      "if (blockIdx.x<1ll ? 1 : 0) {\n",
      "if ((long long)(threadIdx.x) + 512ll * (long long)(blockIdx.x)<14ll ? 1 : 0) {\n",
      "if (blockIdx.x<1ll ? 1 : 0) {\n",
      "        float v = __ldg(tv_ + (long long)(threadIdx.x) + 512ll * (long long)(blockIdx.x));\n",
      "        aten_mul[(long long)(threadIdx.x) + 512ll * (long long)(blockIdx.x)] = v * -3.402823466385289e+38.f;\n",
      "      }    }  }if ((long long)(threadIdx.x) + 512ll * (long long)(blockIdx.x)<2352ll ? 1 : 0) {\n",
      "    float v_1 = __ldg(tattention_scores_1 + (long long)(threadIdx.x) + 512ll * (long long)(blockIdx.x));\n",
      "    float v_2 = __ldg(tv_ + ((long long)(threadIdx.x) + 512ll * (long long)(blockIdx.x)) % 14ll);\n",
      "    aten_add[(long long)(threadIdx.x) + 512ll * (long long)(blockIdx.x)] = v_1 / 8.f + v_2 * -3.402823466385289e+38.f;\n",
      "  }}\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from time import perf_counter\n",
    "\n",
    "# 定义计时函数\n",
    "def timer(f, *args):\n",
    "    torch.cuda.synchronize()  # 确保CUDA操作已完成\n",
    "    start = perf_counter()\n",
    "    f(*args)\n",
    "    torch.cuda.synchronize()  # 确保CUDA操作已完成\n",
    "    return (1000 * (perf_counter() - start))  # 返回毫秒单位的时间\n",
    "\n",
    "# 将模型移到GPU上\n",
    "script_model.cuda()\n",
    "\n",
    "# 将输入张量移到GPU上\n",
    "tokens_tensor_gpu = tokens_tensor.cuda()\n",
    "segments_tensors_gpu = segments_tensors.cuda()\n",
    "\n",
    "# 测试未优化模型的性能\n",
    "native_times = [timer(script_model, tokens_tensor_gpu, segments_tensors_gpu) for _ in range(100)]\n",
    "native_mean_time = np.mean(native_times)\n",
    "print(f\"Average execution time for native model on GPU: {native_mean_time:.2f} ms\")\n",
    "\n",
    "# 生成优化后的模型\n",
    "try:\n",
    "    traced_model = torch.jit.trace(script_model, [tokens_tensor_gpu, segments_tensors_gpu])\n",
    "    \n",
    "    # 测试优化模型的性能\n",
    "    traced_times = [timer(traced_model, tokens_tensor_gpu, segments_tensors_gpu) for _ in range(100)]\n",
    "    traced_mean_time = np.mean(traced_times)\n",
    "    print(f\"Average execution time for traced model on GPU: {traced_mean_time:.2f} ms\")\n",
    "except RuntimeError as e:\n",
    "    print(f\"Tracing failed with error: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
